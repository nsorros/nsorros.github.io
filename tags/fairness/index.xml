<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>fairness on Nick Sorros</title><link>https://nsorros.github.io/tags/fairness/</link><description>Recent content in fairness on Nick Sorros</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 24 Mar 2021 10:10:30 +0300</lastBuildDate><atom:link href="https://nsorros.github.io/tags/fairness/index.xml" rel="self" type="application/rss+xml"/><item><title>Doing algorithmic review as a team: a practical guidance</title><link>https://nsorros.github.io/posts/algorithmic-review-team/</link><pubDate>Wed, 24 Mar 2021 10:10:30 +0300</pubDate><guid>https://nsorros.github.io/posts/algorithmic-review-team/</guid><description>Products that rely on data science run the risk of incorporating societal biases in the algorithms that power them — potentially causing unintended harms to their users. At Wellcome Data Labs we have been thinking and openly sharing our journey towards surfacing and mitigating those harms. We split our review process in two streams
Product impact analysis which looks into harms that can be caused by the product irrespective of the algorithm Algorithmic review process which looks into harms introduced by the model or data irrespective of the product We have written about product impact analysis in the past so this post is going to discuss the second part of our review process in more detail.</description></item><item><title>Assesing the fairness of our machine learning pipeline</title><link>https://nsorros.github.io/posts/assessing-fairness-machine-learning-pipeline/</link><pubDate>Tue, 10 Mar 2020 10:10:30 +0300</pubDate><guid>https://nsorros.github.io/posts/assessing-fairness-machine-learning-pipeline/</guid><description>My day to day job is to develop technologies that automate different processes at Wellcome through data science and machine learning. As a builder of these digital products, my team and I have to consider the unintended consequences they may have on users and on society more broadly. We know this is important because of famous cases where the consequences weren’t considered and harm has been done. For example the Google photos tagging system and Amazon hiring algorithm that have been accused of unintended racist and sexist bias.</description></item></channel></rss>