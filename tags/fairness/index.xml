<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>fairness on Nick Sorros</title><link>https://nsorros.github.io/tags/fairness/</link><description>Recent content in fairness on Nick Sorros</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 10 Mar 2020 10:10:30 +0300</lastBuildDate><atom:link href="https://nsorros.github.io/tags/fairness/index.xml" rel="self" type="application/rss+xml"/><item><title>Assesing the fairness of our machine learning pipeline</title><link>https://nsorros.github.io/posts/assessing-fairness-machine-learning-pipeline/</link><pubDate>Tue, 10 Mar 2020 10:10:30 +0300</pubDate><guid>https://nsorros.github.io/posts/assessing-fairness-machine-learning-pipeline/</guid><description>My day to day job is to develop technologies that automate different processes at Wellcome through data science and machine learning. As a builder of these digital products, my team and I have to consider the unintended consequences they may have on users and on society more broadly. We know this is important because of famous cases where the consequences werenâ€™t considered and harm has been done. For example the Google photos tagging system and Amazon hiring algorithm that have been accused of unintended racist and sexist bias.</description></item></channel></rss>