<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>machine learning on Nick Sorros</title><link>https://nsorros.github.io/tags/machine-learning/</link><description>Recent content in machine learning on Nick Sorros</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 24 Nov 2020 10:10:30 +0300</lastBuildDate><atom:link href="https://nsorros.github.io/tags/machine-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>A neural network tagging biomedical grants</title><link>https://nsorros.github.io/posts/neural-network-tagging-biomedical-grants/</link><pubDate>Tue, 24 Nov 2020 10:10:30 +0300</pubDate><guid>https://nsorros.github.io/posts/neural-network-tagging-biomedical-grants/</guid><description>Neural networks have been a ubiquitous part of the resurgence of Artificial Intelligence over the last few years. Unsurprisingly then, we decided to use a neural network as the modelling approach for tagging our grants with MeSH. Neural networks have raised the state of the art performance on the task to 71% from below 60%. Understandably neural networks may feel complicated to someone outside the field of machine learning, but in this piece my goal is to make them as understandable as logistic regression and Principal Component Analysis (PCA).</description></item><item><title>Assesing the fairness of our machine learning pipeline</title><link>https://nsorros.github.io/posts/assessing-fairness-machine-learning-pipeline/</link><pubDate>Tue, 10 Mar 2020 10:10:30 +0300</pubDate><guid>https://nsorros.github.io/posts/assessing-fairness-machine-learning-pipeline/</guid><description>My day to day job is to develop technologies that automate different processes at Wellcome through data science and machine learning. As a builder of these digital products, my team and I have to consider the unintended consequences they may have on users and on society more broadly. We know this is important because of famous cases where the consequences werenâ€™t considered and harm has been done. For example the Google photos tagging system and Amazon hiring algorithm that have been accused of unintended racist and sexist bias.</description></item><item><title>To Predict or to Explain</title><link>https://nsorros.github.io/posts/to-predict-or-to-explain/</link><pubDate>Wed, 24 May 2017 10:10:30 +0300</pubDate><guid>https://nsorros.github.io/posts/to-predict-or-to-explain/</guid><description>As data scientists, our day job is around modelling. We create models to recommend new products, to increase conversion rates, to explain user behaviour etc. And depending on your background, it is more likely to be familiar either with machine learning techniques or regression type analysis. It is the difference among these two distinct approaches in modelling that motivated me to write this post which is a summary of a talk I gave recently in PyData London.</description></item></channel></rss>