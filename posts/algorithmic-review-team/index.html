<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Doing algorithmic review as a team: a practical guidance | Nick Sorros</title><meta name=keywords content="machine learning,fairness"><meta name=description content="Products that rely on data science run the risk of incorporating societal biases in the algorithms that power them — potentially causing unintended harms to their users. At Wellcome Data Labs we have been thinking and openly sharing our journey towards surfacing and mitigating those harms. We split our review process in two streams
Product impact analysis which looks into harms that can be caused by the product irrespective of the algorithm Algorithmic review process which looks into harms introduced by the model or data irrespective of the product We have written about product impact analysis in the past so this post is going to discuss the second part of our review process in more detail."><meta name=author content><link rel=canonical href=https://nsorros.github.io/posts/algorithmic-review-team/><link crossorigin=anonymous href=/assets/css/stylesheet.bc1149f4a72aa4858d3a9f71462f75e5884ffe8073ea9d6d5761d5663d651e20.css integrity="sha256-vBFJ9KcqpIWNOp9xRi915YhP/oBz6p1tV2HVZj1lHiA=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://nsorros.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://nsorros.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://nsorros.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://nsorros.github.io/apple-touch-icon.png><link rel=mask-icon href=https://nsorros.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-SGMTPX0KNQ"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-SGMTPX0KNQ",{anonymize_ip:!1})}</script><meta property="og:title" content="Doing algorithmic review as a team: a practical guidance"><meta property="og:description" content="Products that rely on data science run the risk of incorporating societal biases in the algorithms that power them — potentially causing unintended harms to their users. At Wellcome Data Labs we have been thinking and openly sharing our journey towards surfacing and mitigating those harms. We split our review process in two streams
Product impact analysis which looks into harms that can be caused by the product irrespective of the algorithm Algorithmic review process which looks into harms introduced by the model or data irrespective of the product We have written about product impact analysis in the past so this post is going to discuss the second part of our review process in more detail."><meta property="og:type" content="article"><meta property="og:url" content="https://nsorros.github.io/posts/algorithmic-review-team/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-03-24T10:10:30+03:00"><meta property="article:modified_time" content="2021-03-24T10:10:30+03:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Doing algorithmic review as a team: a practical guidance"><meta name=twitter:description content="Products that rely on data science run the risk of incorporating societal biases in the algorithms that power them — potentially causing unintended harms to their users. At Wellcome Data Labs we have been thinking and openly sharing our journey towards surfacing and mitigating those harms. We split our review process in two streams
Product impact analysis which looks into harms that can be caused by the product irrespective of the algorithm Algorithmic review process which looks into harms introduced by the model or data irrespective of the product We have written about product impact analysis in the past so this post is going to discuss the second part of our review process in more detail."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://nsorros.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Doing algorithmic review as a team: a practical guidance","item":"https://nsorros.github.io/posts/algorithmic-review-team/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Doing algorithmic review as a team: a practical guidance","name":"Doing algorithmic review as a team: a practical guidance","description":"Products that rely on data science run the risk of incorporating societal biases in the algorithms that power them — potentially causing unintended harms to their users. At Wellcome Data Labs we have been thinking and openly sharing our journey towards surfacing and mitigating those harms. We split our review process in two streams\nProduct impact analysis which looks into harms that can be caused by the product irrespective of the algorithm Algorithmic review process which looks into harms introduced by the model or data irrespective of the product We have written about product impact analysis in the past so this post is going to discuss the second part of our review process in more detail.","keywords":["machine learning","fairness"],"articleBody":"Products that rely on data science run the risk of incorporating societal biases in the algorithms that power them — potentially causing unintended harms to their users. At Wellcome Data Labs we have been thinking and openly sharing our journey towards surfacing and mitigating those harms. We split our review process in two streams\nProduct impact analysis which looks into harms that can be caused by the product irrespective of the algorithm Algorithmic review process which looks into harms introduced by the model or data irrespective of the product We have written about product impact analysis in the past so this post is going to discuss the second part of our review process in more detail.\nWhy do algorithmic review? The goals of the algorithmic review are twofold\nEngage data scientists and the product team in thinking about the potential harms that a machine learning model or algorithmic process in general can cause Facilitate a discussion between the technical and non technical parts of the product team from which different views will surface around the impact certain technical decisions have on the users of the product Who is involved? The algorithmic review, as a more technical kind of review, is primarily completed by the data science team. Certain aspects of the review are about ensuring particular steps were completed e.g. “Is there a plan to protect and secure data?”. These points require less discussion and are more clear cut. Other areas though, most notably agreeing what fairness metric will be used and which groups of users will be evaluated, require careful thought and engagement from the entire product team and, sometimes, subject matter experts outside the team.\nIn terms of roles, the product manager is responsible for ensuring that the review happens and about the steps taken to mitigate certain risks. In organisations where data scientists are not embedded in product teams, that role falls to their respective manager. We also recommend that a member of the technical team takes the role of the fairness lead while a member of the non technical team assumes the role of ethics lead.\nWhen to do it? The review should happen every time the development of a new algorithm begins or before it is deployed and used by users. The benefit of doing the review early is that you can identify harmful issues early. On the other hand, the later you do the review the more information there is available for the actual algorithm being evaluated. The review should also happen in every iteration of the algorithm.\nWhat is involved? At the very least the completion of a checklist (see below) to ensure that different areas that can cause harm have been considered. Often a fairness review follows where a fairness metric is calculated for all relevant groups to investigate potential bias. The process of completing unchecked items to reduce risk is managed by the product owner.\n1. Complete checklist The first step is for the data scientist to complete a pre agreed checklist which covers most areas that need to be scrutinised. We recommend using the DEON checklist.\nThe checklist will ideally live in the same place as the code e.g. Github and will be reviewed in the same way the code is being reviewed in the team e.g. Pull request. The review should include the fairness lead in most items. For certain items it should trigger a discussion with the ethics lead if not the entire product team (see next section).\nFor the items that are left unchecked, actions need to be agreed with the fairness lead and prioritised for completion from the product manager.\n2. Discussing items Certain items in the checklist will undoubtedly need multi disciplinary approval. This is normal and expected. For example deciding whether the data were representative or what metric should be used to evaluate fairness is not a decision that a data scientist could or should make on their own. This is where the ethics lead comes in and either recommends a wider team discussion or agrees on best actions with the data scientist and fairness lead. For example in the DEON checklist we would recommend ensuring that the following items are always discussed with the ethics lead:\nC.1 Missing perspectives: Have we sought to address blindspots in the analysis through engagement with relevant stakeholders (e.g., checking assumptions and discussing implications with affected communities and subject matter experts)? C.2 Dataset bias: Have we examined the data for possible sources of bias and taken steps to mitigate or address these biases (e.g., stereotype perpetuation, confirmation bias, imbalanced classes, or omitted confounding variables)? D.1 Proxy discrimination: Have we ensured that the model does not rely on variables or proxies for variables that are unfairly discriminatory? D.2 Fairness across groups: Have we tested model results for fairness with respect to different affected groups (e.g., tested for disparate error rates)? D.3 Metric selection: Have we considered the effects of optimizing for our defined metrics and considered additional metrics? D.5 Communicate bias: Have we communicated the shortcomings, limitations, and biases of the model to relevant stakeholders in ways that can be generally understood? 3. Fairness review Even though it is not the only thing that avoids harm, a fairness review is at the heart of the algorithmic review. So much so that when we started doing algorithmic reviews we almost exclusively focused on this part before later on extending to more items and using a checklist approach. Core to the fairness review process is defining the metric of interest as well as the groups that will be evaluated.\nDeciding which metric and groups will be used should involve the entire product team as it is highly subjective and application specific. In our case, most of our work involves researchers and the grants they submit, so useful groups for us are the language (e.g. English, French) and the thematic topic (e.g. Neuroscience) of the grant as well as the stage of the researcher’s career (e.g. early/senior).\nMaking a decision around what fairness metric should be used is slightly more challenging as there is a real tradeoff to be made, false positives vs false negatives, and no right answer. As an example in a cancer diagnosis test, a false positive might trigger an unnecessary and hurtful treatment but a false negative would risk missing the cancer. Unfortunately there is often no way to optimise for more than one metric.\n4. Completing items When items are completed from the data scientist, the checklist should be updated and reviewed again. The data scientist should provide information about the steps taken to complete the action. This justification needs to be approved by both the fairness and ethics lead for an item to be considered complete.\nSteps in summary Complete DEON checklist https://deon.drivendata.org Discuss with fairness lead the items on the checklist to come to a mutual agreement about which items should and should not be checked. Ensure that C1, C2 and D1, D2, D3, D5 are not checked if not discussed with ethics and fairness advocates and product manager Agree and prioritise actions, such as perform a fairness review, for unchecked items with fairness lead. Trigger a review e.g. create Github pull request, to incorporate the checklist and actions to the code repository of the project. Product manager should prioritise and assign tasks related to the actions To ensure an action is completed you need to have a discussion and sign off from fairness, ethics lead and the product manager. When an action is completed, alter the checklist, providing justification or analysis for doing so. The change should be reviewed and approved in the same way e.g. pull request by both ethics and fairness leads. Final words I hope you found the post interesting and the process clear. If you work in a team that is doing algorithmic or fairness review, I am curious to hear back on your process. If on the other hand you are part of a team that is interested in reviewing its algorithms and models, I hope you got some ideas on how to do it. If anything was unclear or you want to learn more about our work on the topic, do not hesitate to contact me.\n","wordCount":"1368","inLanguage":"en","datePublished":"2021-03-24T10:10:30+03:00","dateModified":"2021-03-24T10:10:30+03:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://nsorros.github.io/posts/algorithmic-review-team/"},"publisher":{"@type":"Organization","name":"Nick Sorros","logo":{"@type":"ImageObject","url":"https://nsorros.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://nsorros.github.io accesskey=h title="Nick Sorros (Alt + H)">Nick Sorros</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://nsorros.github.io/about/ title=about><span>about</span></a></li><li><a href=https://nsorros.github.io/tags/talk/ title=talks><span>talks</span></a></li><li><a href=https://nsorros.github.io/tags/ title=tags><span>tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://nsorros.github.io>Home</a>&nbsp;»&nbsp;<a href=https://nsorros.github.io/posts/>Posts</a></div><h1 class=post-title>Doing algorithmic review as a team: a practical guidance</h1><div class=post-meta><span title='2021-03-24 10:10:30 +0300 +0300'>March 24, 2021</span>&nbsp;·&nbsp;7 min</div></header><div class=post-content><p>Products that rely on data science run the risk of incorporating societal biases in the algorithms that power them — potentially causing unintended harms to their users. At Wellcome Data Labs we have been thinking and openly sharing our journey towards surfacing and mitigating those harms. We split our review process in two streams</p><ol><li>Product impact analysis which looks into harms that can be caused by the product irrespective of the algorithm</li><li>Algorithmic review process which looks into harms introduced by the model or data irrespective of the product</li></ol><p>We have written about product impact analysis in the <a href=https://medium.com/wellcome-data-labs/doing-impact-analysis-as-a-team-practical-guidance-f68f0427b4e1>past</a> so this post is going to discuss the second part of our review process in more detail.</p><p> 
<img loading=lazy src=/images/ethics-product-development.png#center alt=ethics-product-development>
 </p><h2 id=why-do-algorithmic-review>Why do algorithmic review?<a hidden class=anchor aria-hidden=true href=#why-do-algorithmic-review>#</a></h2><p>The goals of the algorithmic review are twofold</p><ul><li>Engage data scientists and the product team in thinking about the potential harms that a machine learning model or algorithmic process in general can cause</li><li>Facilitate a discussion between the technical and non technical parts of the product team from which different views will surface around the impact certain technical decisions have on the users of the product</li></ul><h2 id=who-is-involved>Who is involved?<a hidden class=anchor aria-hidden=true href=#who-is-involved>#</a></h2><p>The algorithmic review, as a more technical kind of review, is primarily completed by the data science team. Certain aspects of the review are about ensuring particular steps were completed e.g. “Is there a plan to protect and secure data?”. These points require less discussion and are more clear cut. Other areas though, most notably agreeing what <a href=https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb>fairness metric</a> will be used and which groups of users will be evaluated, require careful thought and engagement from the entire product team and, sometimes, subject matter experts outside the team.</p><p>In terms of roles, the product manager is responsible for ensuring that the review happens and about the steps taken to mitigate certain risks. In organisations where data scientists are not embedded in product teams, that role falls to their respective manager. We also recommend that a member of the technical team takes the role of the fairness lead while a member of the non technical team assumes the role of ethics lead.</p><h2 id=when-to-do-it>When to do it?<a hidden class=anchor aria-hidden=true href=#when-to-do-it>#</a></h2><p>The review should happen every time the development of a new algorithm begins or before it is deployed and used by users. The benefit of doing the review early is that you can identify harmful issues early. On the other hand, the later you do the review the more information there is available for the actual algorithm being evaluated. The review should also happen in every iteration of the algorithm.</p><h2 id=what-is-involved>What is involved?<a hidden class=anchor aria-hidden=true href=#what-is-involved>#</a></h2><p>At the very least the completion of a checklist (see below) to ensure that different areas that can cause harm have been considered. Often a fairness review follows where a fairness metric is calculated for all relevant groups to investigate potential bias. The process of completing unchecked items to reduce risk is managed by the product owner.</p><p> 
<img loading=lazy src=/images/deon.png#center alt=deon-checklist>
 </p><h2 id=1-complete-checklist>1. Complete checklist<a hidden class=anchor aria-hidden=true href=#1-complete-checklist>#</a></h2><p>The first step is for the data scientist to complete a pre agreed checklist which covers most areas that need to be scrutinised. We recommend using the <a href=https://deon.drivendata.org/>DEON</a> checklist.</p><p>The checklist will ideally live in the same place as the code e.g. Github and will be reviewed in the same way the code is being reviewed in the team e.g. Pull request. The review should include the fairness lead in most items. For certain items it should trigger a discussion with the ethics lead if not the entire product team (see next section).</p><p>For the items that are left unchecked, actions need to be agreed with the fairness lead and prioritised for completion from the product manager.</p><h2 id=2-discussing-items>2. Discussing items<a hidden class=anchor aria-hidden=true href=#2-discussing-items>#</a></h2><p>Certain items in the checklist will undoubtedly need multi disciplinary approval. This is normal and expected. For example deciding whether the data were representative or what metric should be used to evaluate fairness is not a decision that a data scientist could or should make on their own. This is where the ethics lead comes in and either recommends a wider team discussion or agrees on best actions with the data scientist and fairness lead. For example in the DEON checklist we would recommend ensuring that the following items are always discussed with the ethics lead:</p><ul><li><em>C.1 Missing perspectives</em>: Have we sought to address blindspots in the analysis through engagement with relevant stakeholders (e.g., checking assumptions and discussing implications with affected communities and subject matter experts)?</li><li><em>C.2 Dataset bias</em>: Have we examined the data for possible sources of bias and taken steps to mitigate or address these biases (e.g., stereotype perpetuation, confirmation bias, imbalanced classes, or omitted confounding variables)?</li><li><em>D.1 Proxy discrimination</em>: Have we ensured that the model does not rely on variables or proxies for variables that are unfairly discriminatory?</li><li><em>D.2 Fairness across groups</em>: Have we tested model results for fairness with respect to different affected groups (e.g., tested for disparate error rates)?</li><li><em>D.3 Metric selection</em>: Have we considered the effects of optimizing for our defined metrics and considered additional metrics?</li><li><em>D.5 Communicate bias</em>: Have we communicated the shortcomings, limitations, and biases of the model to relevant stakeholders in ways that can be generally understood?</li></ul><h2 id=3-fairness-review>3. Fairness review<a hidden class=anchor aria-hidden=true href=#3-fairness-review>#</a></h2><p>Even though it is not the only thing that avoids harm, a fairness review is at the heart of the algorithmic review. So much so that when we started doing algorithmic reviews we almost exclusively focused on this part before later on extending to more items and using a checklist approach. Core to the fairness review process is defining the metric of interest as well as the groups that will be evaluated.</p><p>Deciding which metric and groups will be used should involve the entire product team as it is highly subjective and application specific. In our case, most of our work involves researchers and the grants they submit, so useful groups for us are the language (e.g. English, French) and the thematic topic (e.g. Neuroscience) of the grant as well as the stage of the researcher’s career (e.g. early/senior).</p><p>Making a decision around what fairness metric should be used is slightly more challenging as there is a real tradeoff to be made, false positives vs false negatives, and no right answer. As an example in a cancer diagnosis test, a false positive might trigger an unnecessary and hurtful treatment but a false negative would risk missing the cancer. Unfortunately there is often <a href=https://arxiv.org/pdf/1609.05807.pdf>no way to optimise for more than one metric</a>.</p><h2 id=4-completing-items>4. Completing items<a hidden class=anchor aria-hidden=true href=#4-completing-items>#</a></h2><p>When items are completed from the data scientist, the checklist should be updated and reviewed again. The data scientist should provide information about the steps taken to complete the action. This justification needs to be approved by both the fairness and ethics lead for an item to be considered complete.</p><h2 id=steps-in-summary>Steps in summary<a hidden class=anchor aria-hidden=true href=#steps-in-summary>#</a></h2><ol><li>Complete DEON checklist <a href=https://deon.drivendata.org>https://deon.drivendata.org</a></li><li>Discuss with fairness lead the items on the checklist to come to a mutual agreement about which items should and should not be checked. Ensure that C1, C2 and D1, D2, D3, D5 are not checked if not discussed with ethics and fairness advocates and product manager</li><li>Agree and prioritise actions, such as perform a fairness review, for unchecked items with fairness lead.</li><li>Trigger a review e.g. create Github pull request, to incorporate the checklist and actions to the code repository of the project.</li><li>Product manager should prioritise and assign tasks related to the actions</li><li>To ensure an action is completed you need to have a discussion and sign off from fairness, ethics lead and the product manager.</li><li>When an action is completed, alter the checklist, providing justification or analysis for doing so. The change should be reviewed and approved in the same way e.g. pull request by both ethics and fairness leads.</li></ol><h2 id=final-words>Final words<a hidden class=anchor aria-hidden=true href=#final-words>#</a></h2><p>I hope you found the post interesting and the process clear. If you work in a team that is doing algorithmic or fairness review, I am curious to hear back on your process. If on the other hand you are part of a team that is interested in reviewing its algorithms and models, I hope you got some ideas on how to do it. If anything was unclear or you want to learn more about our work on the topic, do not hesitate to contact me.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://nsorros.github.io/tags/machine-learning/>machine learning</a></li><li><a href=https://nsorros.github.io/tags/fairness/>fairness</a></li></ul><nav class=paginav><a class=prev href=https://nsorros.github.io/posts/reproducible-data-science/><span class=title>« Prev</span><br><span>Reproducible data science</span></a>
<a class=next href=https://nsorros.github.io/posts/neural-network-tagging-biomedical-grants/><span class=title>Next »</span><br><span>A neural network tagging biomedical grants</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://nsorros.github.io>Nick Sorros</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>