<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Reproducible data science | Nick Sorros</title><meta name=keywords content="reproducibility"><meta name=description content="Source: https://blog.f1000.com/2014/04/04/reproducibility-tweetchat-recap/ Ever since we started working on data science projects at Wellcome data labs we have been thinking a lot about reproducibility. As a team of data scientists, we wanted to ensure that the results of our work can be recreated by any member of the team. Among other things this allows us to easily collaborate on projects and offer support to each other. It also reduces the time it takes us to transition from an experimental idea to production."><meta name=author content><link rel=canonical href=https://nsorros.github.io/posts/reproducible-data-science/><link crossorigin=anonymous href=/assets/css/stylesheet.bc1149f4a72aa4858d3a9f71462f75e5884ffe8073ea9d6d5761d5663d651e20.css integrity="sha256-vBFJ9KcqpIWNOp9xRi915YhP/oBz6p1tV2HVZj1lHiA=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://nsorros.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://nsorros.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://nsorros.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://nsorros.github.io/apple-touch-icon.png><link rel=mask-icon href=https://nsorros.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Reproducible data science"><meta property="og:description" content="Source: https://blog.f1000.com/2014/04/04/reproducibility-tweetchat-recap/ Ever since we started working on data science projects at Wellcome data labs we have been thinking a lot about reproducibility. As a team of data scientists, we wanted to ensure that the results of our work can be recreated by any member of the team. Among other things this allows us to easily collaborate on projects and offer support to each other. It also reduces the time it takes us to transition from an experimental idea to production."><meta property="og:type" content="article"><meta property="og:url" content="https://nsorros.github.io/posts/reproducible-data-science/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-04-23T10:10:30+03:00"><meta property="article:modified_time" content="2021-04-23T10:10:30+03:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Reproducible data science"><meta name=twitter:description content="Source: https://blog.f1000.com/2014/04/04/reproducibility-tweetchat-recap/ Ever since we started working on data science projects at Wellcome data labs we have been thinking a lot about reproducibility. As a team of data scientists, we wanted to ensure that the results of our work can be recreated by any member of the team. Among other things this allows us to easily collaborate on projects and offer support to each other. It also reduces the time it takes us to transition from an experimental idea to production."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://nsorros.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Reproducible data science","item":"https://nsorros.github.io/posts/reproducible-data-science/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Reproducible data science","name":"Reproducible data science","description":"Source: https://blog.f1000.com/2014/04/04/reproducibility-tweetchat-recap/ Ever since we started working on data science projects at Wellcome data labs we have been thinking a lot about reproducibility. As a team of data scientists, we wanted to ensure that the results of our work can be recreated by any member of the team. Among other things this allows us to easily collaborate on projects and offer support to each other. It also reduces the time it takes us to transition from an experimental idea to production.","keywords":["reproducibility"],"articleBody":" Source: https://blog.f1000.com/2014/04/04/reproducibility-tweetchat-recap/ Ever since we started working on data science projects at Wellcome data labs we have been thinking a lot about reproducibility. As a team of data scientists, we wanted to ensure that the results of our work can be recreated by any member of the team. Among other things this allows us to easily collaborate on projects and offer support to each other. It also reduces the time it takes us to transition from an experimental idea to production.\nHow do we do that? Our process has evolved over the years but the core ideas are heavily inspired by data science cookiecutter. Even though we do not initialise our projects with the template, we re-create the structure as needed in each project. The structure we follow is\nPROJECT_NAME | — data/ | |— raw/ | |— processed/ | — models/ | - [notebooks/] # jupyter notebooks | - [docs/] # documentation can go here including results | - [configs/] | — PROJECT_NAME/ # source code goes here like train.py | — requirements.txt | - Makefile | - README.md | - [setup.py] In square brackets are the files and folders most projects have but we do not enforce.\nData We store our raw data in a dedicated folder and ensure that we keep this data unchanged, and therefore do not invalidate any processing we do after. Whenever we apply any processing to a raw or processed file, the result goes to either the processed (never raw) or models folder depending on the output. Even though we do not apply any versioning to the data or models, something that I will touch again later on the post, this convention allows us to recreate processed files and models by running our code through the raw data.\nOur data and models are synced to AWS S3 (note that s3 provides some form of versoning). Our code is versioned through git and shared through Github. This setup means that any member of the team can switch and work on any project easily by cloning a repo or navigating to the project folder and pulling the data and models from S3.\nMakefile We make the process of syncing data and models even easier through the use of a Makefile.\nPROJECT_NAME := classifier PROJECT_BUCKET := datascience/$(PROJECT_NAME) .PHONY: sync_data sync_data: ## syncs data to s3 aws s3 sync data/ s3://$(PROJECT_BUCKET)/data aws s3 s3://$(PROJECT_BUCKET)/data data/ .PHONY: sync_models sync_models: ## syncs models to s3 aws s3 sync models/ s3://$(PROJECT_BUCKET)/models aws s3 s3://$(PROJECT_BUCKET)/models models/ The Makefile allows us to define shorthand commands for commonly performed actions, and ensures that there is a consistent way to run these commands, which means they can be reproduced with the same result.\nVirtualenv One of the most important components of a data science project which we want to be able to recreate, is the virtualenv that contains all the dependencies of the project (for example the python library ‘pandas’). The dependencies are typically defined in a requirements.txt. To avoid different ways of creating the virtualenv (e.g. python -m venv venv, virtualenv -p python3 venv) and different python and pip versions we use another make command to standardise the creation of the virtualenv.\nPYTHON := python3.8 VENV := venv PIP := venv/bin/pip .PHONY: venv venv: ## creates virtualenv @if [ -d $(VENV) ]; then rm -rf $(VENV); fi @mkdir -p $(VENV) $(PYTHON) -m venv $(VENV) $(PIP) install --upgrade pip $(PIP) install -r requirements.txt Being able to get a reproducible virtualenv still requires that the dependencies you define are pinned to exact versions. Otherwise, every time a user creates a virtualenv with the above command they will get a different set of dependencies. A common way to ensure all required dependencies are pinned is through the use of an unpinned_requirements.txt and pip freeze.\nThe unpinned requirements file consists of the names of unpinned libraries (e.g. pandas) for which you don’t have a preference over which version is installed, and pinned libraries, for which it is important that a specific version or version range is installed (e.g. spacy\u003e2 or spacy==3.0). You can install the unpinned requirements to your virtualenv and then create the requirements.txt with pip freeze which will output all dependencies pinned to the versions installed. You can automate updating the requirements.txt file with yet another make command\nupdate-requirements: VENV := /tmp/update-requirements-venv update-requirements: ## updates requirements @if [ -d $(VENV) ]; then rm -rf $(VENV); fi @mkdir -p $(VENV) $(PYTHON) -m venv $(VENV) $(PIP) install --upgrade pip $(PIP) install -r unpinned_requirements.txt echo \"Created by update-requirements. Do not edit.\" \u003e requirements.txt $(PIP) freeze | grep -v pkg-resources==0.0.0 \u003e\u003e requirements.txt Configs Having uncorrupted raw data and a reproducible virtualenv is only a minimum requirement to be able to reproduce a data science project. We also need a way to replicate the actual analysis or modelling. Fortunately data science projects often follow a standard flow of steps where data is preprocessed, features are extracted, and a model is trained and evaluated. Each of these steps can be defined in a separate file, for example preprocess.py or train.py, and receive command line arguments for the various parameters that are needed to reproduce its outputs. It is important to avoid hardcoding any parameters, even those that seem less likely to change, as when they change you will make your previous results not reproducible. An example train.py would look like:\nimport argparse def train(data_path, model_path, learning_rate, batch_size): ... if __name__ == \"__main__\": argparser = argparse.ArgumentParser() argparser.add_argument(\"--data_path\", type=str, help=\"path to train data\") argparser.add_argument(\"--model_path\", type=str, help=\"path to save the model\") argparser.add_argument(\"--learning_rate\", type=float, help=\"learning rate param\") argparser.add_argument(\"--batch_size\", type=int, help=\"batch size param\") args = argparser.parse_args() data_path = args.data_path model_path = args.model_path learning_rate = args.learning_rate batch_size = args.batch_size train(data_path, model_path, learning_rate, batch_size) Now you can mention the exact command that recreates a certain result in your README and the user can rerun the step easily. You can take this a step further by defining a config file where you define the parameters for all steps of a particular experiment along with a version, something like\n[DEFAULT] version = 2021.03.0 [preprocess] raw_data_path = data/raw/data.xlsx processed_data_path = data/processed/data.jsonl [train] data_path = data/processed/data.jsonl model_path = models/cnn-2021.03.0/ learning_rate = 1e-3 batch_size = 32 and your train.py, changed to read the arguments from the config, would now look like:\nimport configparser import argparse def train(data_path, model_path, learning_rate, batch_size): ... if __name__ == \"__main__\": argparser = argparse.ArgumentParser() argparser.add_argument(\"--config\", type=str, help=\"path to config file\") args = argparser.parse_args() cfg = configReader cfg.read(args.config) data_path = cfg[\"train\"][\"data_path\"] model_path = cfg[\"train\"][\"model_path\"] learning_rate cfg[\"train\"].getfloat(\"learning_rate\") batch_size = cfg[\"train\"].getint(\"batch_size\") train(data_path, model_path, learning_rate, batch_size) In the example shown I am using an INI config file but you can use other formats like TOML, YAML and JSON easily. Config files offer a couple of advantages. The most important for reproducibility is that they contain all the parameters needed for each step in order to recreate the entire experiment. At the same time, they offer a quick way to review all the different parameters tried on a given project. If combined with a results table in a separate document where the version of the config links to the results achieved, someone can get some really good intuition about what has and has not worked. Note that we also use the version number in the model output to be able to know which config produces each model.\nIs that all? The ideas presented so far have taken us a long way to reproducibility but they are not the whole story. I will briefly mention some additional considerations that are also important and that have been progressively playing an important role in our projects.\nTests Possibly the most important next thing is tests. In the absence of tests, to ensure that your code and configs work as expected after every change would require manually rerunning some that are most likely to be affected. Rerunning your configs through your code is a form of integration test so why not automate it? It is also the bare minimum to ensure reproducibility. Ideally though, you want to have unit tests for most steps so that if a change has the potential to affect your results, you are more likely to catch it early.\nCode reviews Another mechanism for catching errors that might impact reproducibility is code reviews. To enable code reviews we work mainly in python scripts instead of notebooks. Code reviews allow us to capture methodological flaws. To reduce the time commitment that code reviews require we aim for pushing small changes, ideally less than 200 lines, even though that is not always possible. Code reviews are also an excellent way to collaborate on projects as data scientists tend to work independently even in teams.\nRandom seeds Data science algorithms are rarely deterministic. From splitting your data to initialising parameters of a neural network, it is typical for some parts of the pipeline to contain a probabilistic component. In order to get reproducible results then, it is important to somehow control that randomness, and the way to do that is by setting up a random seed. Python numpy, tensorflow and most data science libraries expose a standard way to define the random seed that will be used. Note that you may need to set the seed in multiple places in order for it to be truly reproducible.\nData and model versioning Of course the elephant in the room so far has been something I mentioned very early in the post, which is that we do not version our data and models. Actually we sort of version our models as shown earlier by using the version from the config, but what about the data? Well, one idea is to version the data in a similar way to models i.e. introduce a calver number which does not need to follow the config so we do not have to keep identical copies of the data but can change when data has changed. That would be better from what we have now and in fact there is one project that follows that idea but there is still something missing.\nWhat is missing is the fact that, as I mentioned, data science projects constitute a series of steps which can form a Directed Acyclic Graph (DAG). Any change in a file used by the DAG, either data or code, may invalidate the results in the end, and so far we have no way of defining that DAG and checking that no changes have been made. This is solved by DVC, a tool we have lately started using to address that obvious gap in our reproducibility efforts. DVC requires a separate blog post all together so stay tuned.\nYou can see all those ideas in actions, in various stages at the following open source projects (1,2). We would love to hear your thoughts and suggestions on this very important and interesting topic.\n","wordCount":"1799","inLanguage":"en","datePublished":"2021-04-23T10:10:30+03:00","dateModified":"2021-04-23T10:10:30+03:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://nsorros.github.io/posts/reproducible-data-science/"},"publisher":{"@type":"Organization","name":"Nick Sorros","logo":{"@type":"ImageObject","url":"https://nsorros.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://nsorros.github.io accesskey=h title="Nick Sorros (Alt + H)">Nick Sorros</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://nsorros.github.io/about/ title=about><span>about</span></a></li><li><a href=https://nsorros.github.io/tags/talk/ title=talks><span>talks</span></a></li><li><a href=https://nsorros.github.io/tags/ title=tags><span>tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://nsorros.github.io>Home</a>&nbsp;»&nbsp;<a href=https://nsorros.github.io/posts/>Posts</a></div><h1 class=post-title>Reproducible data science</h1><div class=post-meta><span title='2021-04-23 10:10:30 +0300 +0300'>April 23, 2021</span>&nbsp;·&nbsp;9 min</div></header><div class=post-content><p><img loading=lazy src=/images/reproducibility.jpeg#center alt=reproducibility>
Source: <a href=https://blog.f1000.com/2014/04/04/reproducibility-tweetchat-recap/>https://blog.f1000.com/2014/04/04/reproducibility-tweetchat-recap/</a>
 </p><p>Ever since we started working on data science projects at Wellcome data labs we have been thinking a lot about reproducibility. As a team of data scientists, we wanted to ensure that the results of our work can be recreated by any member of the team. Among other things this allows us to easily collaborate on projects and offer support to each other. It also reduces the time it takes us to transition from an experimental idea to production.</p><h2 id=how-do-we-do-that>How do we do that?<a hidden class=anchor aria-hidden=true href=#how-do-we-do-that>#</a></h2><p>Our process has evolved over the years but the core ideas are heavily inspired by <a href=https://drivendata.github.io/cookiecutter-data-science/>data science cookiecutter</a>. Even though we do not initialise our projects with the template, we re-create the structure as needed in each project. The structure we follow is</p><pre tabindex=0><code>PROJECT_NAME
| — data/
|    |— raw/
|    |— processed/
| — models/
| - [notebooks/]       # jupyter notebooks
| - [docs/]            # documentation can go here including results
| - [configs/]
| — PROJECT_NAME/      # source code goes here like train.py
| — requirements.txt
| - Makefile
| - README.md
| - [setup.py]
</code></pre><p>In square brackets are the files and folders most projects have but we do not enforce.</p><h2 id=data>Data<a hidden class=anchor aria-hidden=true href=#data>#</a></h2><p>We store our raw data in a dedicated folder and ensure that we keep this data unchanged, and therefore do not invalidate any processing we do after. Whenever we apply any processing to a raw or processed file, the result goes to either the <code>processed</code> (never raw) or <code>models</code> folder depending on the output. Even though we do not apply any versioning to the data or models, something that I will touch again later on the post, this convention allows us to recreate processed files and models by running our code through the raw data.</p><p>Our data and models are synced to AWS S3 (note that s3 provides some form of versoning). Our code is versioned through git and shared through Github. This setup means that any member of the team can switch and work on any project easily by cloning a repo or navigating to the project folder and pulling the data and models from S3.</p><h2 id=makefile>Makefile<a hidden class=anchor aria-hidden=true href=#makefile>#</a></h2><p>We make the process of syncing data and models even easier through the use of a <a href=https://opensource.com/article/18/8/what-how-makefile>Makefile</a>.</p><pre tabindex=0><code>PROJECT_NAME := classifier
PROJECT_BUCKET := datascience/$(PROJECT_NAME)

.PHONY: sync_data
sync_data: ## syncs data to s3
    aws s3 sync data/ s3://$(PROJECT_BUCKET)/data
    aws s3 s3://$(PROJECT_BUCKET)/data data/

.PHONY: sync_models
sync_models: ## syncs models to s3
    aws s3 sync models/ s3://$(PROJECT_BUCKET)/models
    aws s3 s3://$(PROJECT_BUCKET)/models models/
</code></pre><p>The Makefile allows us to define shorthand commands for commonly performed actions, and ensures that there is a consistent way to run these commands, which means they can be reproduced with the same result.</p><h2 id=virtualenv>Virtualenv<a hidden class=anchor aria-hidden=true href=#virtualenv>#</a></h2><p>One of the most important components of a data science project which we want to be able to recreate, is the <a href=https://virtualenv.pypa.io/en/latest/>virtualenv</a> that contains all the dependencies of the project (for example the python library ‘pandas’). The dependencies are typically defined in a requirements.txt. To avoid different ways of creating the virtualenv (e.g. python -m venv venv, virtualenv -p python3 venv) and different python and pip versions we use another make command to standardise the creation of the virtualenv.</p><pre tabindex=0><code>PYTHON := python3.8
VENV := venv
PIP := venv/bin/pip

.PHONY: venv
venv: ## creates virtualenv
    @if [ -d $(VENV) ]; then rm -rf $(VENV); fi
    @mkdir -p $(VENV)
    $(PYTHON) -m venv $(VENV)
    $(PIP) install --upgrade pip
    $(PIP) install -r requirements.txt
</code></pre><p>Being able to get a reproducible virtualenv still requires that the dependencies you define are pinned to exact versions. Otherwise, every time a user creates a virtualenv with the above command they will get a different set of dependencies. A common way to ensure all required dependencies are pinned is through the use of an <code>unpinned_requirements.txt</code> and <code>pip freeze</code>.</p><p>The unpinned requirements file consists of the names of unpinned libraries (e.g. pandas) for which you don’t have a preference over which version is installed, and pinned libraries, for which it is important that a specific version or version range is installed (e.g. <code>spacy>2</code> or <code>spacy==3.0</code>). You can install the unpinned requirements to your virtualenv and then create the requirements.txt with pip freeze which will output all dependencies pinned to the versions installed. You can automate updating the requirements.txt file with yet another <code>make</code> command</p><pre tabindex=0><code>update-requirements: VENV := /tmp/update-requirements-venv
update-requirements: ## updates requirements
    @if [ -d $(VENV) ]; then rm -rf $(VENV); fi
    @mkdir -p $(VENV)
    $(PYTHON) -m venv $(VENV)
    $(PIP) install --upgrade pip
    $(PIP) install -r unpinned_requirements.txt
    echo &#34;Created by update-requirements. Do not edit.&#34; &gt; requirements.txt
    $(PIP) freeze | grep -v pkg-resources==0.0.0 &gt;&gt; requirements.txt
</code></pre><h2 id=configs>Configs<a hidden class=anchor aria-hidden=true href=#configs>#</a></h2><p>Having uncorrupted raw data and a reproducible virtualenv is only a minimum requirement to be able to reproduce a data science project. We also need a way to replicate the actual analysis or modelling. Fortunately data science projects often follow a standard flow of steps where data is preprocessed, features are extracted, and a model is trained and evaluated. Each of these steps can be defined in a separate file, for example preprocess.py or train.py, and receive command line arguments for the various parameters that are needed to reproduce its outputs. It is important to avoid hardcoding any parameters, even those that seem less likely to change, as when they change you will make your previous results not reproducible. An example train.py would look like:</p><pre tabindex=0><code>import argparse


def train(data_path, model_path, learning_rate, batch_size):
    ...
 

if __name__ == &#34;__main__&#34;:
    argparser = argparse.ArgumentParser()
    argparser.add_argument(&#34;--data_path&#34;, type=str, help=&#34;path to train data&#34;)
    argparser.add_argument(&#34;--model_path&#34;, type=str, help=&#34;path to save the model&#34;)
    argparser.add_argument(&#34;--learning_rate&#34;, type=float, help=&#34;learning rate param&#34;)
    argparser.add_argument(&#34;--batch_size&#34;, type=int, help=&#34;batch size param&#34;)
    args = argparser.parse_args()

    data_path = args.data_path
    model_path = args.model_path
    learning_rate = args.learning_rate
    batch_size = args.batch_size
   
    train(data_path, model_path, learning_rate, batch_size)
</code></pre><p>Now you can mention the exact command that recreates a certain result in your README and the user can rerun the step easily. You can take this a step further by defining a config file where you define the parameters for all steps of a particular experiment along with a version, something like</p><pre tabindex=0><code>[DEFAULT]
version = 2021.03.0

[preprocess]
raw_data_path = data/raw/data.xlsx
processed_data_path = data/processed/data.jsonl

[train]
data_path = data/processed/data.jsonl
model_path = models/cnn-2021.03.0/
learning_rate = 1e-3
batch_size = 32
</code></pre><p>and your train.py, changed to read the arguments from the config, would now look like:</p><pre tabindex=0><code>import configparser
import argparse

def train(data_path, model_path, learning_rate, batch_size):
    ...

if __name__ == &#34;__main__&#34;:
    argparser = argparse.ArgumentParser()
    argparser.add_argument(&#34;--config&#34;, type=str, help=&#34;path to config file&#34;)
    args = argparser.parse_args()

    cfg = configReader
    cfg.read(args.config)
    data_path = cfg[&#34;train&#34;][&#34;data_path&#34;]
    model_path = cfg[&#34;train&#34;][&#34;model_path&#34;]
    learning_rate cfg[&#34;train&#34;].getfloat(&#34;learning_rate&#34;)
    batch_size = cfg[&#34;train&#34;].getint(&#34;batch_size&#34;)

    train(data_path, model_path, learning_rate, batch_size)
</code></pre><p>In the example shown I am using an INI config file but you can use other formats like TOML, YAML and JSON easily. Config files offer a couple of advantages. The most important for reproducibility is that they contain all the parameters needed for each step in order to recreate the entire experiment. At the same time, they offer a quick way to review all the different parameters tried on a given project. If combined with a results table in a separate document where the version of the config links to the results achieved, someone can get some really good intuition about what has and has not worked. Note that we also use the version number in the model output to be able to know which config produces each model.</p><h2 id=is-that-all>Is that all?<a hidden class=anchor aria-hidden=true href=#is-that-all>#</a></h2><p>The ideas presented so far have taken us a long way to reproducibility but they are not the whole story. I will briefly mention some additional considerations that are also important and that have been progressively playing an important role in our projects.</p><h2 id=tests>Tests<a hidden class=anchor aria-hidden=true href=#tests>#</a></h2><p>Possibly the most important next thing is tests. In the absence of tests, to ensure that your code and configs work as expected after every change would require manually rerunning some that are most likely to be affected. Rerunning your configs through your code is a form of integration test so why not automate it? It is also the bare minimum to ensure reproducibility. Ideally though, you want to have unit tests for most steps so that if a change has the potential to affect your results, you are more likely to catch it early.</p><h2 id=code-reviews>Code reviews<a hidden class=anchor aria-hidden=true href=#code-reviews>#</a></h2><p>Another mechanism for catching errors that might impact reproducibility is code reviews. To enable code reviews we work mainly in python scripts instead of notebooks. Code reviews allow us to capture methodological flaws. To reduce the time commitment that code reviews require we aim for pushing small changes, ideally less than 200 lines, even though that is not always possible. Code reviews are also an excellent way to collaborate on projects as data scientists tend to work independently even in teams.</p><h2 id=random-seeds>Random seeds<a hidden class=anchor aria-hidden=true href=#random-seeds>#</a></h2><p>Data science algorithms are rarely deterministic. From splitting your data to initialising parameters of a neural network, it is typical for some parts of the pipeline to contain a probabilistic component. In order to get reproducible results then, it is important to somehow control that randomness, and the way to do that is by setting up a random seed. Python numpy, tensorflow and most data science libraries expose a standard way to define the random seed that will be used. Note that you may need to set the seed in multiple places in order for it to be truly reproducible.</p><h2 id=data-and-model-versioning>Data and model versioning<a hidden class=anchor aria-hidden=true href=#data-and-model-versioning>#</a></h2><p>Of course the elephant in the room so far has been something I mentioned very early in the post, which is that we do not version our data and models. Actually we sort of version our models as shown earlier by using the version from the config, but what about the data? Well, one idea is to version the data in a similar way to models i.e. introduce a calver number which does not need to follow the config so we do not have to keep identical copies of the data but can change when data has changed. That would be better from what we have now and in fact there is one project that follows that idea but there is still something missing.</p><p>What is missing is the fact that, as I mentioned, data science projects constitute a series of steps which can form a Directed Acyclic Graph (DAG). Any change in a file used by the DAG, either data or code, may invalidate the results in the end, and so far we have no way of defining that DAG and checking that no changes have been made. This is solved by <a href=https://dvc.org/>DVC</a>, a tool we have lately started using to address that obvious gap in our reproducibility efforts. DVC requires a separate blog post all together so stay tuned.</p><p>You can see all those ideas in actions, in various stages at the following open source projects (<a href=https://github.com/wellcometrust/nutrition-labels>1</a>,<a href=https://github.com/wellcometrust/grants_tagger>2</a>). We would love to hear your thoughts and suggestions on this very important and interesting topic.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://nsorros.github.io/tags/reproducibility/>reproducibility</a></li></ul><nav class=paginav><a class=prev href=https://nsorros.github.io/posts/tagging-biomedical-grants/><span class=title>« Prev</span><br><span>Tagging biomedical grants with 29K tags</span></a>
<a class=next href=https://nsorros.github.io/posts/algorithmic-review-team/><span class=title>Next »</span><br><span>Doing algorithmic review as a team: a practical guidance</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://nsorros.github.io>Nick Sorros</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>