<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Posts on Nick Sorros</title><link>https://nsorros.github.io/posts/</link><description>Recent content in Posts on Nick Sorros</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 29 Jun 2022 10:10:30 +0300</lastBuildDate><atom:link href="https://nsorros.github.io/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>MLOps with SageMaker ‚Äî Part II</title><link>https://nsorros.github.io/posts/mlops-sagemaker-part2/</link><pubDate>Wed, 29 Jun 2022 10:10:30 +0300</pubDate><guid>https://nsorros.github.io/posts/mlops-sagemaker-part2/</guid><description>Customize train üê≥ In an earlier post we went through how to run a training script using sklearn, PyTorch or transformers with SageMaker by leveraging their preconfigured framework containers. The training scripts we used were self contained, meaning they only used the respective framework and python standard library. This meant we only had to worry about uploading our data and fetching our model from s3, and deciding the instance type we wanted to use.</description></item><item><title>MLOps with SageMaker ‚Äî Part I</title><link>https://nsorros.github.io/posts/mlops-sagemaker-part1/</link><pubDate>Wed, 04 May 2022 10:10:30 +0300</pubDate><guid>https://nsorros.github.io/posts/mlops-sagemaker-part1/</guid><description>How to effortlessly train sklearn üìä, pytorchüî•, and transformers ü§ó models in the cloud SageMaker is a Machine Learning Operations (MLOps) platform, offered by AWS, that provides a number of tools for developing machine learning models from no code solutions to completely custom. With SageMaker, you can label data, train your own models in the cloud using hyperparameter optimization, and then deploy those models easily behind a cloud hosted API. In this series of posts we will explore SageMaker‚Äôs services and provide guides on how to use them, along with code examples.</description></item><item><title>Making an optimisation algorithm 10k times faster üèé</title><link>https://nsorros.github.io/posts/optimisation-10k-faster/</link><pubDate>Wed, 13 Apr 2022 10:10:30 +0300</pubDate><guid>https://nsorros.github.io/posts/optimisation-10k-faster/</guid><description>How we made our multilabel classification threshold optimizer converge in minutes instead of days Multilabel classification is a common task in machine learning and Natural Language Processing (NLP). We approach it by training a model that can apply one or more labels to each new example that it sees. Since the model will output a probability for each of the labels, one of the parameters we can tweak to improve its performance (for example measured in micro f1) is the threshold probability at which a label is applied.</description></item><item><title>Tagging biomedical grants with 29K tags</title><link>https://nsorros.github.io/posts/tagging-biomedical-grants/</link><pubDate>Mon, 13 Dec 2021 10:10:30 +0300</pubDate><guid>https://nsorros.github.io/posts/tagging-biomedical-grants/</guid><description>In a previous post we spoke about a neural architecture we developed for classifying our grants with ~5K disease tags from the MeSH (Medical subject Headings) hierarchy. In this post we will touch on the techniques needed to scale to a model to classify all ~29K MeSH tags. Our dataset consists of 14M biomedical publications labelled with one or more MeSH tags (on average 12 tags per publications), so the challenge is both the thousands of outputs our model needs to recognise and the millions of examples it needs to learn from.</description></item><item><title>Reproducible data science</title><link>https://nsorros.github.io/posts/reproducible-data-science/</link><pubDate>Fri, 23 Apr 2021 10:10:30 +0300</pubDate><guid>https://nsorros.github.io/posts/reproducible-data-science/</guid><description>Source: https://blog.f1000.com/2014/04/04/reproducibility-tweetchat-recap/ Ever since we started working on data science projects at Wellcome data labs we have been thinking a lot about reproducibility. As a team of data scientists, we wanted to ensure that the results of our work can be recreated by any member of the team. Among other things this allows us to easily collaborate on projects and offer support to each other. It also reduces the time it takes us to transition from an experimental idea to production.</description></item><item><title>Doing algorithmic review as a team: a practical guidance</title><link>https://nsorros.github.io/posts/algorithmic-review-team/</link><pubDate>Wed, 24 Mar 2021 10:10:30 +0300</pubDate><guid>https://nsorros.github.io/posts/algorithmic-review-team/</guid><description>Products that rely on data science run the risk of incorporating societal biases in the algorithms that power them ‚Äî potentially causing unintended harms to their users. At Wellcome Data Labs we have been thinking and openly sharing our journey towards surfacing and mitigating those harms. We split our review process in two streams
Product impact analysis which looks into harms that can be caused by the product irrespective of the algorithm Algorithmic review process which looks into harms introduced by the model or data irrespective of the product We have written about product impact analysis in the past so this post is going to discuss the second part of our review process in more detail.</description></item><item><title>A neural network tagging biomedical grants</title><link>https://nsorros.github.io/posts/neural-network-tagging-biomedical-grants/</link><pubDate>Tue, 24 Nov 2020 10:10:30 +0300</pubDate><guid>https://nsorros.github.io/posts/neural-network-tagging-biomedical-grants/</guid><description>Neural networks have been a ubiquitous part of the resurgence of Artificial Intelligence over the last few years. Unsurprisingly then, we decided to use a neural network as the modelling approach for tagging our grants with MeSH. Neural networks have raised the state of the art performance on the task to 71% from below 60%. Understandably neural networks may feel complicated to someone outside the field of machine learning, but in this piece my goal is to make them as understandable as logistic regression and Principal Component Analysis (PCA).</description></item><item><title>Assesing the fairness of our machine learning pipeline</title><link>https://nsorros.github.io/posts/assessing-fairness-machine-learning-pipeline/</link><pubDate>Tue, 10 Mar 2020 10:10:30 +0300</pubDate><guid>https://nsorros.github.io/posts/assessing-fairness-machine-learning-pipeline/</guid><description>My day to day job is to develop technologies that automate different processes at Wellcome through data science and machine learning. As a builder of these digital products, my team and I have to consider the unintended consequences they may have on users and on society more broadly. We know this is important because of famous cases where the consequences weren‚Äôt considered and harm has been done. For example the Google photos tagging system and Amazon hiring algorithm that have been accused of unintended racist and sexist bias.</description></item><item><title>To Predict or to Explain</title><link>https://nsorros.github.io/posts/to-predict-or-to-explain/</link><pubDate>Wed, 24 May 2017 10:10:30 +0300</pubDate><guid>https://nsorros.github.io/posts/to-predict-or-to-explain/</guid><description>As data scientists, our day job is around modelling. We create models to recommend new products, to increase conversion rates, to explain user behaviour etc. And depending on your background, it is more likely to be familiar either with machine learning techniques or regression type analysis. It is the difference among these two distinct approaches in modelling that motivated me to write this post which is a summary of a talk I gave recently in PyData London.</description></item></channel></rss>