<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>MLOps with SageMaker ‚Äî Part II | Nick Sorros</title><meta name=keywords content="machine learning,sagemaker,docker,pytorch,sklearn"><meta name=description content="Customize train üê≥ In an earlier post we went through how to run a training script using sklearn, PyTorch or transformers with SageMaker by leveraging their preconfigured framework containers. The training scripts we used were self contained, meaning they only used the respective framework and python standard library. This meant we only had to worry about uploading our data and fetching our model from s3, and deciding the instance type we wanted to use."><meta name=author content><link rel=canonical href=https://nsorros.github.io/posts/mlops-sagemaker-part2/><link crossorigin=anonymous href=/assets/css/stylesheet.bc1149f4a72aa4858d3a9f71462f75e5884ffe8073ea9d6d5761d5663d651e20.css integrity="sha256-vBFJ9KcqpIWNOp9xRi915YhP/oBz6p1tV2HVZj1lHiA=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://nsorros.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://nsorros.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://nsorros.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://nsorros.github.io/apple-touch-icon.png><link rel=mask-icon href=https://nsorros.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-SGMTPX0KNQ"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-SGMTPX0KNQ",{anonymize_ip:!1})}</script><meta property="og:title" content="MLOps with SageMaker ‚Äî Part II"><meta property="og:description" content="Customize train üê≥ In an earlier post we went through how to run a training script using sklearn, PyTorch or transformers with SageMaker by leveraging their preconfigured framework containers. The training scripts we used were self contained, meaning they only used the respective framework and python standard library. This meant we only had to worry about uploading our data and fetching our model from s3, and deciding the instance type we wanted to use."><meta property="og:type" content="article"><meta property="og:url" content="https://nsorros.github.io/posts/mlops-sagemaker-part2/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-06-29T10:10:30+03:00"><meta property="article:modified_time" content="2022-06-29T10:10:30+03:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="MLOps with SageMaker ‚Äî Part II"><meta name=twitter:description content="Customize train üê≥ In an earlier post we went through how to run a training script using sklearn, PyTorch or transformers with SageMaker by leveraging their preconfigured framework containers. The training scripts we used were self contained, meaning they only used the respective framework and python standard library. This meant we only had to worry about uploading our data and fetching our model from s3, and deciding the instance type we wanted to use."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://nsorros.github.io/posts/"},{"@type":"ListItem","position":3,"name":"MLOps with SageMaker ‚Äî Part II","item":"https://nsorros.github.io/posts/mlops-sagemaker-part2/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"MLOps with SageMaker ‚Äî Part II","name":"MLOps with SageMaker ‚Äî Part II","description":"Customize train üê≥ In an earlier post we went through how to run a training script using sklearn, PyTorch or transformers with SageMaker by leveraging their preconfigured framework containers. The training scripts we used were self contained, meaning they only used the respective framework and python standard library. This meant we only had to worry about uploading our data and fetching our model from s3, and deciding the instance type we wanted to use.","keywords":["machine learning","sagemaker","docker","pytorch","sklearn"],"articleBody":"Customize train üê≥ In an earlier post we went through how to run a training script using sklearn, PyTorch or transformers with SageMaker by leveraging their preconfigured framework containers. The training scripts we used were self contained, meaning they only used the respective framework and python standard library. This meant we only had to worry about uploading our data and fetching our model from s3, and deciding the instance type we wanted to use.\nThis is excellent for quick prototyping but in practice our code lives in separate modules to reduce duplication, and often requires additional libraries. Case in point: all our training scripts from the previous post were sharing the same load_data function and our PyTorch script had to implement its own tokenizer in the absence of additional libraries.\nIn this post, we will look into how we can customize SageMaker training to address those issues as well as create our own containers to run training.\nAs a reminder, this is our minimal project setup The code for our projects lives in src and the libraries required are defined in requirements.txt.\nThe data comes from a sentiment classification task in Kaggle. The data comes in CSV format and contains two columns, text and label. The label is either 0 or 1.\nWe use a scripts folder to add all necessary functionality for interacting with SageMaker. The intention is that you can simply take some of those into your own project and get started.\nHere is an sklearn training script that uses tf-idf naive-bayes which imports load_data from a utils module. And here is the utils module Let‚Äôs run this outside of SageMaker first. In order for SageMaker to run our job, we need to include any additional files that are required. We can do that via the source_dir parameter that all SageMaker Estimators (e.g. SkLearn, PyTorch, HuggingFace, etc.) expose. As the name hints, this allows us to specify a directory with our source code which should be included. This is our src folder.\nHere is the code for running the job, which is very similar to one from the previous post\nNotice that the entry_point no longer needs to include src as the path is now relative. This is how you run it\nThis should take care of any additional modules our code might depend on. Remember we need to prepend data and model paths with file:// or s3:// for local or s3 paths respectively.\nHow about extra libraries? In our previous post our pytorch training script included a custom tokenizer to avoid using external libraries. Here is another pytorch training script which uses the tokenizers library from hugging face for tokenization instead.\nThe only thing we have to change for SageMaker to pick up our additional libraries is an environment variable called SAGEMAKER_REQUIREMENTS. This variable needs to point to our requirements.txt. We can define environment variables to be passed inside the container using the environment parameter of our estimator.\nHere is the part that needs to change (full script)\nWhen SageMaker runs our job it will install all additional libraries mentioned in that file during setup.\nWhile these customisations can take you a long way, they are still building on top of SageMaker preconfigured containers. This is convenient because we do not have to build and push any containers to run our job or worry about setting up gpu drivers. There are cases though, for example when you are not using one of the frameworks for which there are preconfigured containers, where a custom container environment for your job makes more sense.\nTo customize the training environment, we need to create our own containers. We need a Dockerfile for that. Here is a minimal one to use with our sklearn scripts.\nNotice that we do not pass any code. The intention is to create an environment with all libraries needed and pass the training script and any additional source files when invoking the job, similar to what we have been doing with the preconfigured containers. This is called Script Mode mode in SageMaker, and in order for it to work we need to have sagemaker-training installed. Let‚Äôs build our container\nTo run our job we will use a generic Estimator that is framework agnostic\nThe main difference is that we need to pass the image_uri parameter which is the name of our container, script-sklearn in this case.\nTo be able to run the job using AWS infrastructure we need to push our container to ECR. Here is a small bash script that does this (read more here)\nThis approach allows us to customize our environment but we can take it even further if we remove sagemaker-training. This means we need to replicate some of its functionality. Let‚Äôs start with the Dockerfile\nNotice that we need to copy our code. This makes it slightly less flexible as we will need to rebuild and push our container whenever we make any change to the training. We also need to name our entrypoint python script train, and make it executable intentionally because SageMaker will look for and run train upon start, and it will fail if it cannot find it. Note that we also update the PYTHONPATH to make our code visible from the train entrypoint script so that the imports work.\nHere is our python entrypoint:\nIt is important to include #!/usr/bin/env python so that SageMaker knows how to run the script. In the absence of sagemaker-training, the paths are not initialized in environment variables and the hyperparameters are not read and passed automatically, so we need to do that ourselves. The predefined locations that SageMaker used are\n/opt/ml/input/data/train ‚Äî this is where the train data is being copied, /opt/ml/model ‚Äî the contents of this folder are copied out of the container, /opt/ml/input/config/hyperparameters.json ‚Äî this is where the parameters passed as arguments to the estimator are saved We also use Pydantic to validate and convert types for our hyperparameters, otherwise we would need to convert each to their right type.\nYou can find all the code in our sagemaker examples repository https://github.com/MantisAI/sagemaker_examples\nIn this post we explored how to customize our SageMaker training job by including additional files and defining any external dependencies. We also show how to use our own custom containers so we are not limited to the ones provided by SageMaker. This post builds on top of an earlier post which also defines some helper script on how to move data and models to and from AWS as well as monitor training which might be of interest to you.\nIn the next post, we will explore SageMaker inference which reduces the complexity around deploying our models behind an endpoint significantly.\n","wordCount":"1113","inLanguage":"en","datePublished":"2022-06-29T10:10:30+03:00","dateModified":"2022-06-29T10:10:30+03:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://nsorros.github.io/posts/mlops-sagemaker-part2/"},"publisher":{"@type":"Organization","name":"Nick Sorros","logo":{"@type":"ImageObject","url":"https://nsorros.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://nsorros.github.io accesskey=h title="Nick Sorros (Alt + H)">Nick Sorros</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://nsorros.github.io/about/ title=about><span>about</span></a></li><li><a href=https://nsorros.github.io/tags/talk/ title=talks><span>talks</span></a></li><li><a href=https://nsorros.github.io/tags/ title=tags><span>tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://nsorros.github.io>Home</a>&nbsp;¬ª&nbsp;<a href=https://nsorros.github.io/posts/>Posts</a></div><h1 class=post-title>MLOps with SageMaker ‚Äî Part II</h1><div class=post-meta><span title='2022-06-29 10:10:30 +0300 +0300'>June 29, 2022</span>&nbsp;¬∑&nbsp;6 min</div></header><div class=post-content><h2 id=customize-train->Customize train üê≥<a hidden class=anchor aria-hidden=true href=#customize-train->#</a></h2><p>In an earlier <a href=https://medium.com/mantisnlp/mlops-with-sagemaker-44ffc2c1054a>post</a> we went through how to run a training script using <code>sklearn</code>, <code>PyTorch</code> or <code>transformers</code> with SageMaker by leveraging their preconfigured framework containers. The training scripts we used were self contained, meaning they only used the respective framework and python standard library. This meant we only had to worry about uploading our data and fetching our model from s3, and deciding the instance type we wanted to use.</p><p>This is excellent for quick prototyping but in practice our code lives in separate modules to reduce duplication, and often requires additional libraries. Case in point: all our training scripts from the previous post were sharing the same <code>load_data</code> function and our PyTorch script had to implement its own <code>tokenizer</code> in the absence of additional libraries.</p><p>In this post, we will look into how we can customize SageMaker training to address those issues as well as create our own containers to run training.</p><p>As a reminder, this is our minimal project setup
<img loading=lazy src=/images/project-structure.png alt=project-structure></p><p>The code for our projects lives in src and the libraries required are defined in <code>requirements.txt</code>.</p><p>The data comes from a s<a href=https://www.kaggle.com/yasserh/imdb-movie-ratings-sentiment-analysis>entiment classification task</a> in Kaggle. The data comes in CSV format and contains two columns, text and label. The label is either 0 or 1.</p><p>We use a scripts folder to add all necessary functionality for interacting with SageMaker. The intention is that you can simply take some of those into your own project and get started.</p><p>Here is an <code>sklearn</code> training script that uses <code>tf-idf naive-bayes</code> which imports <code>load_data</code> from a <code>utils</code> module.
¬†
<img loading=lazy src=/images/tfidf-naivebayes-script.png#center alt=tfidf-naivebayes-script>
¬†</p><p>And here is the utils module
¬†
<img loading=lazy src=/images/load-data.png#center alt=load-data>
¬†</p><p>Let‚Äôs run this outside of SageMaker first.
¬†
<img loading=lazy src=/images/run-train-sklearn-naivebayes.png#center alt=run-train-sklearn-naivebayes>
¬†</p><p>In order for SageMaker to run our job, we need to include any additional files that are required. We can do that via the <code>source_dir</code> parameter that all SageMaker Estimators (e.g. SkLearn, PyTorch, HuggingFace, etc.) expose. As the name hints, this allows us to specify a directory with our source code which should be included. This is our src folder.</p><p>Here is the code for running the job, which is very similar to one from the previous <a href=https://github.com/MantisAI/sagemaker_examples/blob/main/src/train_sklearn.py>post</a></p><p>¬†
<img loading=lazy src=/images/sagemaker-sklearn-naivebayes.png#center alt=sagemaker-sklearn-naivebayes>
¬†</p><p>Notice that the <code>entry_point</code> no longer needs to include src as the path is now relative. This is how you run it</p><p>¬†
<img loading=lazy src=/images/run-sagemaker-sklearn-naivebayes.png#center alt=run-sagemaker-sklearn-naivebayes>
¬†</p><p>This should take care of any additional modules our code might depend on. Remember we need to prepend data and model paths with <code>file://</code> or <code>s3://</code> for local or s3 paths respectively.</p><p>How about extra libraries? In our previous <a href=https://medium.com/mantisnlp/mlops-with-sagemaker-44ffc2c1054a>post</a> our pytorch training <a href=https://github.com/MantisAI/sagemaker_examples/blob/main/src/train_pytorch.py>script</a> included a custom tokenizer to avoid using external libraries. Here is another pytorch training script which uses the tokenizers library from hugging face for tokenization instead.</p><p>¬†
<img loading=lazy src=/images/pytorch-script-transformers-tokenizer.png#center alt=pytorch-script-transformers-tokenizer>
¬†</p><p>The only thing we have to change for SageMaker to pick up our additional libraries is an environment variable called <code>SAGEMAKER_REQUIREMENTS</code>. This variable needs to point to our r<code>equirements.txt</code>. We can define <code>environment</code> variables to be passed inside the container using the environment parameter of our estimator.</p><p>Here is the part that needs to change (<a href=https://github.com/MantisAI/sagemaker_examples/blob/main/scripts/run_pytorch_transformers_tokenizer_sagemaker.py>full script</a>)</p><p>¬†
<img loading=lazy src=/images/sagemaker-pytorch-environment.png#center alt=sagemaker-pytorch-environment>
¬†</p><p>When SageMaker runs our job it will install all additional libraries mentioned in that file during setup.</p><p>While these customisations can take you a long way, they are still building on top of SageMaker preconfigured containers. This is convenient because we do not have to build and push any containers to run our job or worry about setting up gpu drivers. There are cases though, for example when you are not using one of the frameworks for which there are preconfigured containers, where a custom container environment for your job makes more sense.</p><p>To customize the training environment, we need to create our own containers. We need a Dockerfile for that. Here is a minimal one to use with our sklearn scripts.</p><p>¬†
<img loading=lazy src=/images/train-dockerfile.png#center alt=train-dockerfile>
¬†</p><p>Notice that we do not pass any code. The intention is to create an environment with all libraries needed and pass the training script and any additional source files when invoking the job, similar to what we have been doing with the preconfigured containers. This is called Script Mode mode in SageMaker, and in order for it to work we need to have <code>sagemaker-training</code> installed. Let‚Äôs build our container</p><p>¬†
<img loading=lazy src=/images/docker-build-sklearn.png#center alt=docker-build-sklearn>
¬†</p><p>To run our job we will use a generic Estimator that is framework agnostic</p><p>¬†
<img loading=lazy src=/images/sagemaker-train-estimator.png#center alt=sagemaker-train-estimator>
¬†</p><p>The main difference is that we need to pass the image_uri parameter which is the name of our container, script-sklearn in this case.</p><p>¬†
<img loading=lazy src=/images/run-sagemaker-train-estimator.png#center alt=run-sagemaker-train-estimator>
¬†</p><p>To be able to run the job using AWS infrastructure we need to push our container to ECR. Here is a small bash script that does this (read more <a href=https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html>here</a>)</p><p>¬†
<img loading=lazy src=/images/upload-container.png#center alt=upload-container>
¬†</p><p>This approach allows us to customize our environment but we can take it even further if we remove <code>sagemaker-training</code>. This means we need to replicate some of its functionality. Let‚Äôs start with the Dockerfile</p><p>¬†
<img loading=lazy src=/images/dockerfile-custom.png#center alt=dockerfile-custom>
¬†</p><p>Notice that we need to copy our code. This makes it slightly less flexible as we will need to rebuild and push our container whenever we make any change to the training. We also need to name our entrypoint python script train, and make it executable intentionally because SageMaker will look for and run train upon start, and it will fail if it cannot find it. Note that we also update the <code>PYTHONPATH</code> to make our code visible from the train entrypoint script so that the imports work.</p><p>Here is our python entrypoint:</p><p>¬†
<img loading=lazy src=/images/train-entrypoint.png#center alt=train-entrypoint>
¬†</p><p>It is important to include <code>#!/usr/bin/env</code> python so that SageMaker knows how to run the script. In the absence of <code>sagemaker-training</code>, the paths are not initialized in environment variables and the hyperparameters are not read and passed automatically, so we need to do that ourselves. The predefined locations that SageMaker used are</p><ul><li><code>/opt/ml/input/data/train</code> ‚Äî this is where the train data is being copied,</li><li><code>/opt/ml/model</code> ‚Äî the contents of this folder are copied out of the container,</li><li><code>/opt/ml/input/config/hyperparameters.json</code> ‚Äî this is where the parameters passed as arguments to the estimator are saved</li></ul><p>We also use <code>Pydantic</code> to validate and convert types for our hyperparameters, otherwise we would need to convert each to their right type.</p><p>You can find all the code in our sagemaker examples repository <a href=https://github.com/MantisAI/sagemaker_examples>https://github.com/MantisAI/sagemaker_examples</a></p><p>In this post we explored how to customize our SageMaker training job by including additional files and defining any external dependencies. We also show how to use our own custom containers so we are not limited to the ones provided by SageMaker. This post builds on top of an earlier post which also defines some helper script on how to move data and models to and from AWS as well as monitor training which might be of interest to you.</p><p>In the next post, we will explore SageMaker inference which reduces the complexity around deploying our models behind an endpoint significantly.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://nsorros.github.io/tags/machine-learning/>machine learning</a></li><li><a href=https://nsorros.github.io/tags/sagemaker/>sagemaker</a></li><li><a href=https://nsorros.github.io/tags/docker/>docker</a></li><li><a href=https://nsorros.github.io/tags/pytorch/>pytorch</a></li><li><a href=https://nsorros.github.io/tags/sklearn/>sklearn</a></li></ul><nav class=paginav><a class=next href=https://nsorros.github.io/posts/extreme-multilabel-classification-talk/><span class=title>Next ¬ª</span><br><span>Extreme Multilabel Classification in the Biomedical NLP Domain</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://nsorros.github.io>Nick Sorros</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>