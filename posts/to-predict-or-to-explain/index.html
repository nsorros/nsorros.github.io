<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>To Predict or to Explain | Nick Sorros</title><meta name=keywords content="machine learning,explanatory modelling"><meta name=description content="As data scientists, our day job is around modelling. We create models to recommend new products, to increase conversion rates, to explain user behaviour etc. And depending on your background, it is more likely to be familiar either with machine learning techniques or regression type analysis. It is the difference among these two distinct approaches in modelling that motivated me to write this post which is a summary of a talk I gave recently in PyData London."><meta name=author content><link rel=canonical href=https://nsorros.github.io/posts/to-predict-or-to-explain/><link crossorigin=anonymous href=/assets/css/stylesheet.bc1149f4a72aa4858d3a9f71462f75e5884ffe8073ea9d6d5761d5663d651e20.css integrity="sha256-vBFJ9KcqpIWNOp9xRi915YhP/oBz6p1tV2HVZj1lHiA=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://nsorros.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://nsorros.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://nsorros.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://nsorros.github.io/apple-touch-icon.png><link rel=mask-icon href=https://nsorros.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="To Predict or to Explain"><meta property="og:description" content="As data scientists, our day job is around modelling. We create models to recommend new products, to increase conversion rates, to explain user behaviour etc. And depending on your background, it is more likely to be familiar either with machine learning techniques or regression type analysis. It is the difference among these two distinct approaches in modelling that motivated me to write this post which is a summary of a talk I gave recently in PyData London."><meta property="og:type" content="article"><meta property="og:url" content="https://nsorros.github.io/posts/to-predict-or-to-explain/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2017-05-24T10:10:30+03:00"><meta property="article:modified_time" content="2017-05-24T10:10:30+03:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="To Predict or to Explain"><meta name=twitter:description content="As data scientists, our day job is around modelling. We create models to recommend new products, to increase conversion rates, to explain user behaviour etc. And depending on your background, it is more likely to be familiar either with machine learning techniques or regression type analysis. It is the difference among these two distinct approaches in modelling that motivated me to write this post which is a summary of a talk I gave recently in PyData London."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://nsorros.github.io/posts/"},{"@type":"ListItem","position":3,"name":"To Predict or to Explain","item":"https://nsorros.github.io/posts/to-predict-or-to-explain/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"To Predict or to Explain","name":"To Predict or to Explain","description":"As data scientists, our day job is around modelling. We create models to recommend new products, to increase conversion rates, to explain user behaviour etc. And depending on your background, it is more likely to be familiar either with machine learning techniques or regression type analysis. It is the difference among these two distinct approaches in modelling that motivated me to write this post which is a summary of a talk I gave recently in PyData London.","keywords":["machine learning","explanatory modelling"],"articleBody":"As data scientists, our day job is around modelling. We create models to recommend new products, to increase conversion rates, to explain user behaviour etc. And depending on your background, it is more likely to be familiar either with machine learning techniques or regression type analysis. It is the difference among these two distinct approaches in modelling that motivated me to write this post which is a summary of a talk I gave recently in PyData London.\nGenerally speaking, we model so as to achieve one of two goals a) explain what is going on in our observations, or b) predict given an unseen observation. In a perfect, noise free world, these two endeavours would converge to the same model, which would be the model that produces the data we observe but in reality there are different trade-offs to be made depending on the goal of your model.\nThese differences arise from two main areas:\nthe operationalisation of theoretical constructs the bias-variance trade-off Theoretical constructs, such as intelligence, give us the ability to explain and articulate our causal hypothesis. At the same time, in order to create a model, we need operationalised versions of these constructs which are measurable, such as I.Q. score, in order to do modelling. It is that distance between the theoretical construct and its equivalent measurable definition that justifies part of the difference between the two approaches.\nAt the same time when doing modelling, we seek to minimise the expected error. This can be decomposed in two terms, the Bias and the Variance. Bias represents the distance between the actual model producing the data and the one we use while variance captures the complexity of the model. When our goal is to do statistical modelling to test a causal hypothesis, i.e. explain, we focus on minimising the distance between the actual model and the one we constructed, thus minimise bias. On the other hand when predicting we care mostly about minimising the combined bias and variance, and sometimes this means sacrificing model accuracy, thus increasing bias, for minimising variance, an example of which is regularisation.\nWhile these are the high level reasons why statistical modelling for prediction and explanation are different, these differences manifest in different ways in all steps of creating a model. The process of building a model in machine learning looks like this:\nStarting from the pre processing step, the volume of data used tends to be dramatically different. On the one hand, explanatory modelling utilises as little data as possible in order to produce statistically significant results and thus one of the first steps usually is to calculate the sample size needed for that. Predictive modelling, on the other hand, is significantly more data hungry, since every additional data point can have a significant impact on the ability of the model to generalise which translates into increased predictive accuracy.\nFeature engineering surfaces two more differences. Features (operationalised constructs) in explanatory modelling have a very specific role, and that is to test an underlying theory or hypothesis. This means that any transformation of the variables that makes them uninterpretable, such as PCA, or addition of unrelated to the theory variables is avoided since it does not increase our ability to explain even if it increases our ability to predict. The other difference has to do with the availability of a feature at prediction time. When modelling for prediction, it only makes sense to use features that are available at the time we wish to make the prediction, a restriction that does not apply when our goal is to best explain our observable world.\nDuring the modelling step, the choice of model is governed heavily from the underlying goal of modelling. This is because different models score differently in their ability to explain versus their ability to predict. On the one end of the spectrum, neural networks are powerful models with the ability to model complex relationships from the dataset but which offer us with little intuition on how they work, and on the other end, linear regression is a very simple model that rarely performs well in prediction but gives us a very clear view on the contribution of the different features.\nThe final difference comes into play during the model evaluation step. It should come as not surprise at this point that there are different ways to evaluate performance in each case. Explanatory modelling asks for a way to assess explanatory power and finds it usually in the form of RÂ² which quantifies the amount of variance explained by our model. Predictive modelling cares only for predictive power and thus quantifies success by using predictive accuracy.\nAt this point, I hope that I have convinced you that modelling for prediction is not the same as modelling for explanation which leads me to my two take away points:\npredictive modelling techniques perform better at prediction tasks there are scientific and ethical reasons to explain our black box techniques The first point might sound obvious but there are a number of cases where explanatory techniques are being used to make predictions, for example election forecasting, something that often results in poor results.\nThe second point is a bit more nuanced but what I mean is that if we want to better expand our understanding of how nature works, it will not be enough to create models that are good in replicating nature. Neural networks offer a perfect example for this, because even though they have been proven to work better than the human brain in quite complex tasks, they have not assisted much in understanding how our brain works. The ethical dimension comes from the fact, that more and more lately, we are using algorithms to replace human decision making, and even though humans come with their own biases, this is no excuse for replicating these biases into the algorithms we develop. We do not want models that are neither racial nor sexually biased to make hiring or legal decisions whereas at the moment we have both.\nI leave you with a final thought, next time you develop a model why not ask first, what is the goal of this model? To predict or to explain? And then use the most relevant technique for the task.\nFor more details you can:\nwatch my PyData talk read the paper that inspired me to give the talk watch the presentation of the author of the paper ","wordCount":"1070","inLanguage":"en","datePublished":"2017-05-24T10:10:30+03:00","dateModified":"2017-05-24T10:10:30+03:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://nsorros.github.io/posts/to-predict-or-to-explain/"},"publisher":{"@type":"Organization","name":"Nick Sorros","logo":{"@type":"ImageObject","url":"https://nsorros.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://nsorros.github.io accesskey=h title="Nick Sorros (Alt + H)">Nick Sorros</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://nsorros.github.io/about/ title=about><span>about</span></a></li><li><a href=https://nsorros.github.io/tags/talk/ title=talks><span>talks</span></a></li><li><a href=https://nsorros.github.io/tags/ title=tags><span>tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://nsorros.github.io>Home</a>&nbsp;Â»&nbsp;<a href=https://nsorros.github.io/posts/>Posts</a></div><h1 class=post-title>To Predict or to Explain</h1><div class=post-meta><span title='2017-05-24 10:10:30 +0300 +0300'>May 24, 2017</span>&nbsp;Â·&nbsp;6 min</div></header><div class=post-content><p>As data scientists, our day job is around modelling. We create models to recommend new products, to increase conversion rates, to explain user behaviour etc. And depending on your background, it is more likely to be familiar either with machine learning techniques or regression type analysis. It is the difference among these two distinct approaches in modelling that motivated me to write this post which is a summary of a talk I gave recently in PyData London.</p><p>Generally speaking, we model so as to achieve one of two goals a) <em>explain</em> what is going on in our observations, or b) <em>predict</em> given an unseen observation. In a perfect, noise free world, these two endeavours would converge to the same model, which would be the model that produces the data we observe but in reality there are different trade-offs to be made depending on the goal of your model.</p><p>These differences arise from two main areas:</p><ul><li>the <em>operationalisation</em> of theoretical <em>constructs</em></li><li>the <em>bias-variance</em> trade-off</li></ul><p>Theoretical constructs, such as intelligence, give us the ability to explain and articulate our causal hypothesis. At the same time, in order to create a model, we need operationalised versions of these constructs which are measurable, such as I.Q. score, in order to do modelling. It is that distance between the theoretical construct and its equivalent measurable definition that justifies part of the difference between the two approaches.</p><p>At the same time when doing modelling, we seek to minimise the expected error. This can be decomposed in two terms, the Bias and the Variance. Bias represents the distance between the actual model producing the data and the one we use while variance captures the complexity of the model. When our goal is to do statistical modelling to test a causal hypothesis, i.e. explain, we focus on minimising the distance between the actual model and the one we constructed, thus minimise bias. On the other hand when predicting we care mostly about minimising the combined bias and variance, and sometimes this means sacrificing model accuracy, thus increasing bias, for minimising variance, an example of which is regularisation.</p><p>Â 
<img loading=lazy src=/images/bias-variance.png#center alt=bias-variance>
Â </p><p>While these are the high level reasons why statistical modelling for prediction and explanation are different, these differences manifest in different ways in all steps of creating a model. The process of building a model in machine learning looks like this:</p><p>Â 
<img loading=lazy src=/images/ml-process.png#center alt=ml-process>
Â </p><p>Starting from the <em>pre processing</em> step, the volume of data used tends to be dramatically different. On the one hand, explanatory modelling utilises as little data as possible in order to produce statistically significant results and thus one of the first steps usually is to calculate the sample size needed for that. Predictive modelling, on the other hand, is significantly more data hungry, since every additional data point can have a significant impact on the ability of the model to generalise which translates into increased predictive accuracy.</p><p><em>Feature engineering</em> surfaces two more differences. Features (operationalised constructs) in explanatory modelling have a very specific role, and that is to test an underlying theory or hypothesis. This means that any transformation of the variables that makes them uninterpretable, such as PCA, or addition of unrelated to the theory variables is avoided since it does not increase our ability to explain even if it increases our ability to predict. The other difference has to do with the availability of a feature at prediction time. When modelling for prediction, it only makes sense to use features that are available at the time we wish to make the prediction, a restriction that does not apply when our goal is to best explain our observable world.</p><p>During the <em>modelling</em> step, the choice of model is governed heavily from the underlying goal of modelling. This is because different models score differently in their ability to explain versus their ability to predict. On the one end of the spectrum, neural networks are powerful models with the ability to model complex relationships from the dataset but which offer us with little intuition on how they work, and on the other end, linear regression is a very simple model that rarely performs well in prediction but gives us a very clear view on the contribution of the different features.</p><p>Â 
<img loading=lazy src=/images/predict-explain.png alt=predict-explain>
Â </p><p>The final difference comes into play during the <em>model evaluation</em> step. It should come as not surprise at this point that there are different ways to evaluate performance in each case. Explanatory modelling asks for a way to assess explanatory power and finds it usually in the form of RÂ² which quantifies the amount of variance explained by our model. Predictive modelling cares only for predictive power and thus quantifies success by using predictive accuracy.</p><p>At this point, I hope that I have convinced you that modelling for prediction is not the same as modelling for explanation which leads me to my two take away points:</p><ul><li>predictive modelling techniques perform better at prediction tasks</li><li>there are scientific and ethical reasons to explain our black box techniques</li></ul><p>The first point might sound obvious but there are a number of cases where explanatory techniques are being used to make predictions, for example election forecasting, something that often results in poor results.</p><p>The second point is a bit more nuanced but what I mean is that if we want to better expand our understanding of how nature works, it will not be enough to create models that are good in replicating nature. Neural networks offer a perfect example for this, because even though they have been proven to work better than the human brain in quite complex tasks, they have not assisted much in understanding how our brain works. The ethical dimension comes from the fact, that more and more lately, we are using algorithms to replace human decision making, and even though humans come with their own biases, this is no excuse for replicating these biases into the algorithms we develop. We do not want models that are neither racial nor sexually biased to make hiring or legal decisions whereas at the moment we have both.</p><p>I leave you with a final thought, next time you develop a model why not ask first, what is the goal of this model? To predict or to explain? And then use the most relevant technique for the task.</p><p>For more details you can:</p><ul><li><a href="https://www.youtube.com/watch?v=3ywnb3W-hNU&t=2s">watch my PyData talk</a></li><li><a href=https://arxiv.org/pdf/1101.0891.pdf>read the paper that inspired me to give the talk</a></li><li><a href="https://www.youtube.com/watch?v=vWH_HNfQVRI">watch the presentation of the author of the paper</a></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://nsorros.github.io/tags/machine-learning/>machine learning</a></li><li><a href=https://nsorros.github.io/tags/explanatory-modelling/>explanatory modelling</a></li></ul><nav class=paginav><a class=prev href=https://nsorros.github.io/posts/assessing-fairness-machine-learning-pipeline/><span class=title>Â« Prev</span><br><span>Assesing the fairness of our machine learning pipeline</span></a>
<a class=next href=https://nsorros.github.io/posts/to-predict-or-to-explain-talk/><span class=title>Next Â»</span><br><span>To explain or to predict?</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://nsorros.github.io>Nick Sorros</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>